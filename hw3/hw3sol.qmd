---
title: "Biostat 212A Homework 3"
subtitle: "Due Feb 20, 2024 @ 11:59PM"
author: "Yang An and UID:106332601"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 5.4.2 (10pts)
2. We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.
(a) What is the probability that the first bootstrap observation is not the jth observation from the original sample? 

The probability that the first bootstrap observation is not the jth observation from the original sample is 1 - 1/n. This is because the probability of the first bootstrap observation being the jth observation from the original sample is 1/n. Therefore, the probability that the first bootstrap observation is not the jth observation from the original sample is 1 - 1/n.


(b) What is the probability that the second bootstrap observation is not the jth observation from the original sample?

The probability that the second bootstrap observation is not the jth observation from the original sample is 1 - 1/n. This is because the probability of the second bootstrap observation being the jth observation from the original sample is 1/n. Therefore, the probability that the second bootstrap observation is not the jth observation from the original sample is 1 - 1/n.
(c) Argue that the probability that the jth observation is not in the bootstrap sample is (1 − 1/n)^n.

The probability that the jth observation is not in the bootstrap sample is (1 − 1/n)^n. This is because the probability that the first bootstrap observation is not the jth observation from the original sample is 1 - 1/n. The probability that the second bootstrap observation is not the jth observation from the original sample is 1 - 1/n. Therefore, the probability that the jth observation is not in the bootstrap sample is (1 − 1/n)^n.

(d) When n = 5, what is the probability that the jth observation is in the bootstrap sample?

When n = 5, the probability that the jth observation is in the bootstrap sample is 1-(1 − 1/5)^5 = 0.67232.

(e) When n = 100, what is the probability that the jth observation is in the bootstrap sample?

When n = 100, the probability that the jth observation is in the bootstrap sample is 1-(1 − 1/100)^100 = 0.6339676587267709.

(f) When n = 10, 000, what is the probability that the jth observa- tion is in the bootstrap sample?

When n = 10, 000, the probability that the jth observation is in the bootstrap sample is 1-(1 − 1/10,000)^10,000 = 0.6321.

(g) Create a plot that displays, for each integer value of n from 1 to 100,000, the probability that the jth observation is in the bootstrap sample. Comment on what you observe.

```{r}
library(ggplot2)
boot1 <- data.frame(n = 1:100000)
boot1$prob <- 1 - (1 - 1/boot1$n)^boot1$n
ggplot(data = boot1, aes(x = n, y = prob)) +
  geom_line() +
  labs(title = "Probability that the jth observation is in the bootstrap sample",
       x = "n",
       y = "Probability") +
  theme_bw() +
  theme(text = element_text(size = 8))

```
The plot shows that the probability that the jth observation is in the bootstrap sample decreases as n increases. The probability that the jth observation is in the bootstrap sample is 1 when n = 1, and it decreases to 0.6321 when n = 10,000. 

(h) We will now investigate numerically the probability that a boot- strap sample of size n = 100 contains the jth observation. Here j = 4. We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.
Comment on the results obtained.

```{r}
set.seed(1)
boot2 <- rep(0, 10000)
for (i in 1:10000) {
 boot2[i] <- sum(sample(1:100, replace = TRUE) %in% 4) > 0
}
mean(boot2)

```
The probability that a bootstrap sample of size n = 100 contains the jth observation is 0.6417. This is close to the probability that the jth observation is in the bootstrap sample when n = 100, which is 0.6339676587267709.


## ISL Exercise 5.4.9 (20pts)

## Least squares is MLE (10pts)

Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and $C_p$ and AIC are equivalent.

## ISL Exercise 6.6.1 (10pts)

## ISL Exercise 6.6.3 (10pts)

## ISL Exercise 6.6.4 (10pts)

## ISL Exercise 6.6.5 (10pts)

## ISL Exercise 6.6.11 (30pts)

You must follow the [typical machine learning paradigm](https://ucla-econ-425t.github.io/2023winter/slides/06-modelselection/workflow_lasso.html) to compare _at least_ 3 methods: least squares, lasso, and ridge. Report final results as

| Method | CV RMSE | Test RMSE |
|:------:|:------:|:------:|:------:|
| LS | | | |
| Ridge | | | |
| Lasso | | | |
| ... | | | |

## Bonus question (20pts)

Consider a linear regression, fit by least squares to a set of training data $(x_1, y_1), \ldots, (x_N,  y_N)$ drawn at random from a population. Let $\hat \beta$ be the least squares estimate. Suppose we have some test data $(\tilde{x}_1, \tilde{y}_1), \ldots, (\tilde{x}_M, \tilde{y}_M)$ drawn at random from the same population as the training data. If $R_{\text{train}}(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2$ and $R_{\text{test}}(\beta) = \frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \beta^T \tilde{x}_i)^2$. Show that
$$
\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].
$$