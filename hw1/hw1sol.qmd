---
title: "Biostat 212a Homework 1"
subtitle: "Due Jan 23, 2024 @ 11:59PM"
author: "Yang An, UID: 106332601"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## Filling gaps in lecture notes (10pts)

Consider the regression model
$$
Y = f(X) + \epsilon,
$$
where $\operatorname{E}(\epsilon) = 0$. 

### Optimal regression function

Show that the choice
$$
f_{\text{opt}}(X) = \operatorname{E}(Y | X)
$$
minimizes the mean squared prediction error
$$
\operatorname{E}\{[Y - f(X)]^2\},
$$
where the expectations averages over variations in both $X$ and $Y$. (Hint: condition on $X$.)
**Answer** 
$$ \begin{aligned} \operatorname{E}\{[Y - f(X)]^2\} &= \operatorname{E}\{[Y - \operatorname{E}(Y | X) + \operatorname{E}(Y | X) - f(X)]^2\} \\ &= \operatorname{E}\{[Y - \operatorname{E}(Y | X)]^2\} + \operatorname{E}\{[\operatorname{E}(Y | X) - f(X)]^2\} \\ &= \operatorname{Var}(Y | X) + \operatorname{E}\{[\operatorname{E}(Y | X) - f(X)]^2\} \end{aligned} $$
Since$\operatorname{Var}(Y | X) \geq 0$, the minimum is achieved when $\operatorname{E}\{[\operatorname{E}(Y | X) - f(X)]^2\} = 0$, which is equivalent to $f(X) = \operatorname{E}(Y | X)$. 


### Bias-variance trade-off

Given an estimate $\hat f$ of $f$, show that the test error at a $x_0$ can be decomposed as
$$
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} = \underbrace{\operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2}_{\text{MSE of } \hat f(x_0) \text{ for estimating } f(x_0)} + \underbrace{\operatorname{Var}(\epsilon)}_{\text{irreducible}},
$$
where the expectation averages over the variability in $y_0$ and $\hat f$.
**Answer** 
$$ \begin{aligned} \operatorname{E}\{[y_0 - \hat f(x_0)]^2\} &= \operatorname{E}\{[y_0 - f(x_0) + f(x_0) - \hat f(x_0)]^2\} \\ &= \operatorname{E}\{[y_0 - f(x_0)]^2\} + \operatorname{E}\{[f(x_0) - \hat f(x_0)]^2\} \\ &= \operatorname{Var}(y_0) + \operatorname{E}\{[f(x_0) - \hat f(x_0)]^2\} \\ &= \operatorname{Var}(y_0) + \operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2 \end{aligned} $$
where the last equality is due to the fact that
$$\operatorname{E}\{[f(x_0) - \hat f(x_0)]^2\} = \operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2$$. 



## ISL Exercise 2.4.3 (10pts)

**Answer** 
（a)

```{r, eval = F}
library(tidyverse)
x <- seq(0.0, 10.0, 0.02)

squared_bias <- function(x) {
  return(0.002*(-x+10)^3)
}
variance <- function(x) {
  return(0.002*x^3)
}
training_error <- function(x) {
  return(2.38936 - 0.825077*x + 0.176655*x^2 - 0.0182319*x^3 + 0.00067091*x^4)
}
test_error <- function(x) {
  return(3 - 0.6*x + 0.06*x^2)
}
bayes_error <- function(x) {
  return(x + 1 - x)
}

plot(x, squared_bias(x), type='l', col='red', xlab='the amount of flexibility', ylab='values', ylim=c(0, 4), main='Bias-Variance Tradeoff')
lines(x, variance(x), col='blue')
lines(x, training_error(x), col='green')
lines(x, test_error(x), col='orange')
lines(x, bayes_error(x), col='purple')
legend('top', legend=c('Squared Bias', 'Variance', 'Training Error', 'Test Error', 'Bayes Error'), col=c('red', 'blue', 'green', 'orange', 'purple'), lty=1, cex=0.5)

```


（b) 
1. Bias: The bias curve typically decreases as the flexibility of the method increases. This is because more flexible methods can better fit the training data, reducing the bias.
2. Variance: The variance curve typically increases as the flexibility of the method increases. This is because more flexible methods are more prone to overfitting, leading to higher variance.
3. Training Error: The training error curve typically decreases as the flexibility of the method increases. This is because more flexible methods can better fit the training data, resulting in lower training error.
4. Test Error: The test error curve typically forms a U-shape, with a minimum point at an intermediate level of flexibility. This is because less flexible methods have high bias but low variance, leading to underfitting and high test error. More flexible methods have low bias but high variance, leading to overfitting and high test error. The optimal level of flexibility balances the bias and variance, resulting in the lowest test error.
5. Bayes Error: The Bayes (or irreducible) error curve is a horizontal line and represents the inherent error that cannot be reduced by any statistical learning method. It does not change with the flexibility of the method.

## ISL Exercise 2.4.4 (10pts)



## ISL Exercise 2.4.10 (30pts)

Your can read in the `boston` data set directly from url <https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv>. A documentation of the `boston` data set is [here](https://www.rdocumentation.org/packages/ISLR2/versions/1.3-2/topics/Boston).

::: {.panel-tabset}

#### R

```{r, evalue = F}
library(tidyverse)

Boston <- read_csv("https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv", col_select = -1) %>% 
  print(width = Inf)
```



:::

## ISL Exercise 3.7.3 (12pts)

## ISL Exercise 3.7.15 (20pts)

## Bonus question (20pts)

For multiple linear regression, show that $R^2$ is equal to the correlation between the response vector $\mathbf{y} = (y_1, \ldots, y_n)^T$ and the fitted values $\hat{\mathbf{y}} = (\hat y_1, \ldots, \hat y_n)^T$. That is
$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = [\operatorname{Cor}(\mathbf{y}, \hat{\mathbf{y}})]^2.
$$

