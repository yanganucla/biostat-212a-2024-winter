---
title: "Biostat 212a Homework 1"
subtitle: "Due Jan 23, 2024 @ 11:59PM"
author: "Yang An, UID: 106332601"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## Filling gaps in lecture notes (10pts)

Consider the regression model
$$
Y = f(X) + \epsilon,
$$
where $\operatorname{E}(\epsilon) = 0$. 

### Optimal regression function

Show that the choice
$$
f_{\text{opt}}(X) = \operatorname{E}(Y | X)
$$
minimizes the mean squared prediction error
$$
\operatorname{E}\{[Y - f(X)]^2\},
$$
where the expectations averages over variations in both $X$ and $Y$. (Hint: condition on $X$.)
**Answer** 
$$ \begin{aligned} \operatorname{E}\{[Y - f(X)]^2\} &= \operatorname{E}\{[Y - \operatorname{E}(Y | X) + \operatorname{E}(Y | X) - f(X)]^2\} \\ &= \operatorname{E}\{[Y - \operatorname{E}(Y | X)]^2\} + \operatorname{E}\{[\operatorname{E}(Y | X) - f(X)]^2\} \\ &= \operatorname{Var}(Y | X) + \operatorname{E}\{[\operatorname{E}(Y | X) - f(X)]^2\} \end{aligned} $$
Since$\operatorname{Var}(Y | X) \geq 0$, the minimum is achieved when $\operatorname{E}\{[\operatorname{E}(Y | X) - f(X)]^2\} = 0$, which is equivalent to $f(X) = \operatorname{E}(Y | X)$. 


### Bias-variance trade-off

Given an estimate $\hat f$ of $f$, show that the test error at a $x_0$ can be decomposed as
$$
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} = \underbrace{\operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2}_{\text{MSE of } \hat f(x_0) \text{ for estimating } f(x_0)} + \underbrace{\operatorname{Var}(\epsilon)}_{\text{irreducible}},
$$
where the expectation averages over the variability in $y_0$ and $\hat f$.
**Answer** 
$$ \begin{aligned} \operatorname{E}\{[y_0 - \hat f(x_0)]^2\} &= \operatorname{E}\{[y_0 - f(x_0) + f(x_0) - \hat f(x_0)]^2\} \\ &= \operatorname{E}\{[y_0 - f(x_0)]^2\} + \operatorname{E}\{[f(x_0) - \hat f(x_0)]^2\} \\ &= \operatorname{Var}(y_0) + \operatorname{E}\{[f(x_0) - \hat f(x_0)]^2\} \\ &= \operatorname{Var}(y_0) + \operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2 \end{aligned} $$
where the last equality is due to the fact that
$$\operatorname{E}\{[f(x_0) - \hat f(x_0)]^2\} = \operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2$$. 



## ISL Exercise 2.4.3 (10pts)
**Answer** 
（a)

```{r, eval = F}
library(tidyverse)
x <- seq(0.0, 10.0, 0.02)

squared_bias <- function(x) {
  return(0.002*(-x+10)^3)
}
variance <- function(x) {
  return(0.002*x^3)
}
training_error <- function(x) {
  return(2.38936 - 0.825077*x + 0.176655*x^2 - 0.0182319*x^3 + 0.00067091*x^4)
}
test_error <- function(x) {
  return(3 - 0.6*x + 0.06*x^2)
}
bayes_error <- function(x) {
  return(x + 1 - x)
}

plot(x, squared_bias(x), type='l', col='red', xlab='the amount of flexibility', ylab='values', ylim=c(0, 4), main='Bias-Variance Tradeoff')
lines(x, variance(x), col='blue')
lines(x, training_error(x), col='green')
lines(x, test_error(x), col='orange')
lines(x, bayes_error(x), col='purple')
legend('top', legend=c('Squared Bias', 'Variance', 'Training Error', 'Test Error', 'Bayes Error'), col=c('red', 'blue', 'green', 'orange', 'purple'), lty=1, cex=0.5)

```


（b) 
1. Bias: The bias curve typically decreases as the flexibility of the method increases. This is because more flexible methods can better fit the training data, reducing the bias.
2. Variance: The variance curve typically increases as the flexibility of the method increases. This is because more flexible methods are more prone to overfitting, leading to higher variance.
3. Training Error: The training error curve typically decreases as the flexibility of the method increases. This is because more flexible methods can better fit the training data, resulting in lower training error.
4. Test Error: The test error curve typically forms a U-shape, with a minimum point at an intermediate level of flexibility. This is because less flexible methods have high bias but low variance, leading to underfitting and high test error. More flexible methods have low bias but high variance, leading to overfitting and high test error. The optimal level of flexibility balances the bias and variance, resulting in the lowest test error.
5. Bayes Error: The Bayes (or irreducible) error curve is a horizontal line and represents the inherent error that cannot be reduced by any statistical learning method. It does not change with the flexibility of the method.

## ISL Exercise 2.4.4 (10pts)
**Answer**
(a) 
1. Spam detection: The response is whether an email is spam or not. The predictors are the words in the email. The goal is prediction.
2. Credit card fraud detection: The response is whether a transaction is fraudulent or not. The predictors are the transaction amount, location, time, etc. The goal is prediction.
3. Medical diagnosis: The response is whether a patient has a certain disease or not. The predictors are the patient’s symptoms, test results, etc. The goal is inference.
(b)
1. House price prediction: The response is the price of a house. The predictors are the house’s location, size, number of bedrooms, etc. The goal is prediction.
2. Stock price prediction: The response is the price of a stock. The predictors are the stock’s historical prices, trading volume, etc. The goal is prediction.
3. Weather forecasting: The response is the weather conditions. The predictors are the weather conditions in the past. The goal is prediction.
(c)
1. Market segmentation: The goal is to divide customers into groups based on their purchasing behavior.
2. Social network analysis: The goal is to identify groups of people with similar interests.
3. Image segmentation: The goal is to identify objects in an image.



## ISL Exercise 2.4.10 (30pts)

#### R

**Answer**（a)
There are 506 rows and 13 columns in this data set. Each row represents a census tract in Boston, and each column represents a predictor (or response).

```{r, evalue = F}
library(tidyverse)

Boston <- read_csv("https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv", col_select = -1) %>% 
  print(width = Inf)
```

#### R
**Answer**（b)
I explored some other scatterplots not shown below. In general there aren’t many easily interpretable relationships between variables in this data set. Some pairs have clusters of data, but no overall trend; others have floor effects where most communities have a value of zero (e.g., crime).

```{r, eval = F}
ggscatter <- function(x, y) {
  ggplot(boston, aes({{ x }}, {{ y }})) +
    geom_point()
}

ggscatter(chas, crim) +
  labs(
    title = "Crime occurs more often away from the Charles River",
    subtitle = "Or, most suburbs are away from the Charles River"
  )
ggscatter(zn, nox) +
  labs(
    title = paste0(
      "Higher nitrogen oxide concentration in suburbs with all residential \n",
      "land zoning below 25,000 sq.ft.")
  )
ggscatter(ptratio, medv) +
  labs(
    title = paste0(
      "Higher pupil to teacher ratios are related to less \n",
      "valuable owner-occupied homes"
    )
  )


``` 

#### R
**Answer**(c)
Yes, lower socioeconomic status and less valuable homes are associated with higher crime rates.
```{r, eval = F}
ggscatter(crim, lstat) +
  labs(
    title = "Higher crime rates are associated with lower socioeconomic status"
  )
ggscatter(crim, medv) +
  labs(
    title = "Higher crime rates are associated with less valuable homes"
  )

```


(d) Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.
(e) How many of the census tracts in this data set bound the Charles river?
(f) What is the median pupil-teacher ratio among the towns in this data set?
(g) Which census tract of Boston has lowest median value of owner- occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.
(h) In this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.
Your can read in the `boston` data set directly from url <https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv>. A documentation of the `boston` data set is [here](https://www.rdocumentation.org/packages/ISLR2/versions/1.3-2/topics/Boston).



## ISL Exercise 3.7.3 (12pts)

## ISL Exercise 3.7.15 (20pts)

## Bonus question (20pts)

For multiple linear regression, show that $R^2$ is equal to the correlation between the response vector $\mathbf{y} = (y_1, \ldots, y_n)^T$ and the fitted values $\hat{\mathbf{y}} = (\hat y_1, \ldots, \hat y_n)^T$. That is
$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = [\operatorname{Cor}(\mathbf{y}, \hat{\mathbf{y}})]^2.
$$

