---
title: "Biostat 212a Homework 1"
subtitle: "Due Jan 23, 2024 @ 11:59PM"
author: "Yang An, UID: 106332601"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## Filling gaps in lecture notes (10pts)

Consider the regression model
$$
Y = f(X) + \epsilon,
$$
where $\operatorname{E}(\epsilon) = 0$. 

### Optimal regression function

Show that the choice
$$
f_{\text{opt}}(X) = \operatorname{E}(Y | X)
$$
minimizes the mean squared prediction error
$$
\operatorname{E}\{[Y - f(X)]^2\},
$$
where the expectations averages over variations in both $X$ and $Y$. (Hint: condition on $X$.)
**Answer** 
$$ \begin{aligned} \operatorname{E}\{[Y - f(X)]^2\} &= \operatorname{E}\{[Y - \operatorname{E}(Y | X) + \operatorname{E}(Y | X) - f(X)]^2\} \\ &= \operatorname{E}\{[Y - \operatorname{E}(Y | X)]^2\} + \operatorname{E}\{[\operatorname{E}(Y | X) - f(X)]^2\} \\ &= \operatorname{Var}(Y | X) + \operatorname{E}\{[\operatorname{E}(Y | X) - f(X)]^2\} \end{aligned} $$
Since$\operatorname{Var}(Y | X) \geq 0$, the minimum is achieved when $\operatorname{E}\{[\operatorname{E}(Y | X) - f(X)]^2\} = 0$, which is equivalent to $f(X) = \operatorname{E}(Y | X)$. 


### Bias-variance trade-off

Given an estimate $\hat f$ of $f$, show that the test error at a $x_0$ can be decomposed as
$$
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} = \underbrace{\operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2}_{\text{MSE of } \hat f(x_0) \text{ for estimating } f(x_0)} + \underbrace{\operatorname{Var}(\epsilon)}_{\text{irreducible}},
$$
where the expectation averages over the variability in $y_0$ and $\hat f$.
**Answer** 
$$ \begin{aligned} \operatorname{E}\{[y_0 - \hat f(x_0)]^2\} &= \operatorname{E}\{[y_0 - f(x_0) + f(x_0) - \hat f(x_0)]^2\} \\ &= \operatorname{E}\{[y_0 - f(x_0)]^2\} + \operatorname{E}\{[f(x_0) - \hat f(x_0)]^2\} \\ &= \operatorname{Var}(y_0) + \operatorname{E}\{[f(x_0) - \hat f(x_0)]^2\} \\ &= \operatorname{Var}(y_0) + \operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2 \end{aligned} $$
where the last equality is due to the fact that
$$\operatorname{E}\{[f(x_0) - \hat f(x_0)]^2\} = \operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2$$. 



## ISL Exercise 2.4.3 (10pts)
**Answer** 
（a)

```{r, eval = F}
library(tidyverse)
x <- seq(0.0, 10.0, 0.02)

squared_bias <- function(x) {
  return(0.002*(-x+10)^3)
}
variance <- function(x) {
  return(0.002*x^3)
}
training_error <- function(x) {
  return(2.38936 - 0.825077*x + 0.176655*x^2 - 0.0182319*x^3 + 0.00067091*x^4)
}
test_error <- function(x) {
  return(3 - 0.6*x + 0.06*x^2)
}
bayes_error <- function(x) {
  return(x + 1 - x)
}

plot(x, squared_bias(x), type='l', col='red', xlab='the amount of flexibility', ylab='values', ylim=c(0, 4), main='Bias-Variance Tradeoff')
lines(x, variance(x), col='blue')
lines(x, training_error(x), col='green')
lines(x, test_error(x), col='orange')
lines(x, bayes_error(x), col='purple')
legend('top', legend=c('Squared Bias', 'Variance', 'Training Error', 'Test Error', 'Bayes Error'), col=c('red', 'blue', 'green', 'orange', 'purple'), lty=1, cex=0.5)

```


（b) 
1. Bias: The bias curve typically decreases as the flexibility of the method increases. This is because more flexible methods can better fit the training data, reducing the bias.
2. Variance: The variance curve typically increases as the flexibility of the method increases. This is because more flexible methods are more prone to overfitting, leading to higher variance.
3. Training Error: The training error curve typically decreases as the flexibility of the method increases. This is because more flexible methods can better fit the training data, resulting in lower training error.
4. Test Error: The test error curve typically forms a U-shape, with a minimum point at an intermediate level of flexibility. This is because less flexible methods have high bias but low variance, leading to underfitting and high test error. More flexible methods have low bias but high variance, leading to overfitting and high test error. The optimal level of flexibility balances the bias and variance, resulting in the lowest test error.
5. Bayes Error: The Bayes (or irreducible) error curve is a horizontal line and represents the inherent error that cannot be reduced by any statistical learning method. It does not change with the flexibility of the method.

## ISL Exercise 2.4.4 (10pts)
**Answer**
(a) 
1. Spam detection: The response is whether an email is spam or not. The predictors are the words in the email. The goal is prediction.
2. Credit card fraud detection: The response is whether a transaction is fraudulent or not. The predictors are the transaction amount, location, time, etc. The goal is prediction.
3. Medical diagnosis: The response is whether a patient has a certain disease or not. The predictors are the patient’s symptoms, test results, etc. The goal is inference.
(b)
1. House price prediction: The response is the price of a house. The predictors are the house’s location, size, number of bedrooms, etc. The goal is prediction.
2. Stock price prediction: The response is the price of a stock. The predictors are the stock’s historical prices, trading volume, etc. The goal is prediction.
3. Weather forecasting: The response is the weather conditions. The predictors are the weather conditions in the past. The goal is prediction.
(c)
1. Market segmentation: The goal is to divide customers into groups based on their purchasing behavior.
2. Social network analysis: The goal is to identify groups of people with similar interests.
3. Image segmentation: The goal is to identify objects in an image.



## ISL Exercise 2.4.10 (30pts)

#### R

**Answer**（a)
There are 506 rows and 13 columns in this data set. Each row represents a census tract in Boston, and each column represents a predictor (or response).

```{r, evalue = F}
library(tidyverse)

Boston <- read_csv("https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv", col_select = -1) %>% 
  print(width = Inf)
```

#### R
**Answer**（b)
I explored some other scatterplots not shown below. In general there aren’t many easily interpretable relationships between variables in this data set. Some pairs have clusters of data, but no overall trend; others have floor effects where most communities have a value of zero (e.g., crime).

```{r, eval = F}
ggscatter <- function(x, y) {
  ggplot(boston, aes({{ x }}, {{ y }})) +
    geom_point()
}

ggscatter(chas, crim) +
  labs(
    title = "Crime occurs more often away from the Charles River",
    subtitle = "Or, most suburbs are away from the Charles River"
  )
ggscatter(zn, nox) +
  labs(
    title = paste0(
      "Higher nitrogen oxide concentration in suburbs with all residential \n",
      "land zoning below 25,000 sq.ft.")
  )
ggscatter(ptratio, medv) +
  labs(
    title = paste0(
      "Higher pupil to teacher ratios are related to less \n",
      "valuable owner-occupied homes"
    )
  )


``` 

#### R
**Answer**(c)
Yes, lower socioeconomic status and less valuable homes are associated with higher crime rates.
```{r, eval = F}
ggscatter(crim, lstat) +
  labs(
    title = "Higher crime rates are associated with lower socioeconomic status"
  )
ggscatter(crim, medv) +
  labs(
    title = "Higher crime rates are associated with less valuable homes"
  )

```

#### R
**Answer**
Yes, some census tracts have particularly high crime rates, tax rates, and pupil-teacher ratios. The range of each predictor is shown below.
```{r, eval = F}
# draw histograms
attach(new_df)
par(mfrow=c(1,3))
hist(crim, breaks=10, col="2")
hist(tax, breaks=10, col="3")
hist(ptratio, breaks=10, col="4")
# Select only the variables
new_df <- select(Boston, crim, tax, ptratio)
# Calculate the range for each numerical column
range_num_vars <- lapply(new_df, range)
# Print the range for each numerical column
range_num_vars
```


#### R
**Answer**(e)
There are 35 census tracts that bound the Charles River.
```{r, eval = F}
boston %>% 
  mutate(bound = if_else(chas == 1, "Yes", "No")) %>% 
  group_by(bound) %>% 
  summarise(
    n = n()
  )
```


#### R
**Answer**(f)
The median pupil-teacher ratio is 19.05.
```{r, eval = F}
boston %>% 
  summarise(
    median_ptratio = median(ptratio)
  )
```

#### R
**Answer**（g)
The census tract with the lowest median value of owner-occupied homes is 399. The values of the other predictors for that census tract are shown below. 
crim: above 3rd quartile
zn: at min
indus: at 3rd quartile
chas: not bounded by river
nox: bove 3rd quartile
rm: below 1st quartile
age: at max
dis: below 1st quartile
rad: at max
tax: at 3rd quartile
ptratio: at 3rd quartile
black: 399 at max; 406 above 1st quartile
lstat: above 3rd quartile
medv: at min
```{r}
# the below will display the first record
lowest_record <- Boston[which.min(Boston$medv),]
print(lowest_record)
# to display all the recods with the minimum value
t(subset(Boston, medv == min(medv)))
summary(Boston)
```


#### R
**Answer**(h)
There are 64 census tracts that average more than seven rooms per dwelling, and 13 census tracts that average more than eight rooms per dwelling. The census tracts that average more than eight rooms per dwelling have a median value of owner-occupied homes that is higher than the overall median value of owner-occupied homes. 
```{r, eval = F}
boston %>% 
  mutate(
    more_than_7_rooms = if_else(rm > 7, "Yes", "No"),
    more_than_8_rooms = if_else(rm > 8, "Yes", "No")
  ) %>% 
  group_by(more_than_7_rooms, more_than_8_rooms) %>% 
  summarise(
    n = n(),
    median_medv = median(medv),
    median_crim = median(crim)
  )

```



## ISL Exercise 3.7.3 (12pts)
**Answer** 
(a) 
iii is true.
i:the equation to Salary = 50 + 20 * GPA + 0.07 * IQ + 35 * College + 0.01 * GPA * IQ - 10 * GPA * College
We now can estimate that High School graduates earn an average of 50 + 20 * mean(GPA) + 0.07 * mean(IQ) + 0.01 * mean(GPA) * mean(IQ) and College graduates earn an average of 50 + 20 * mean(GPA) + 0.07 * mean(IQ) + 35 + 0.01 * mean(GPA) * mean(IQ) - 10 * mean(GPA). When you subtract out the common terms, you find out that College graduates earn an average of 35 - 10 * mean(GPA) more than High School graduates. Since we don't know the value of mean(GPA), we don't know whether High School graduates are outearning College graduates on average or not.
ii:Again, this is indeterminate.
iii:Since College are earning an average of 35 - 10 * mean(GPA) more than men, a higher GPA mean means that College earn less than high school graduates, so this is true.
iv:Since college are earning an average of 35 - 10 * mean(GPA) more than high school graduates, a higher GPA mean means that college earn less than high school graduates, so this is false.

**Answer** 
(b) 137.1 (50 + 20*4 + 0.07*110 + 35 + 0.01*4*110 - 10*4)

**Answer** 
(c) This is false, because the statistical significance of an interaction is different from the magnitude of the interaction. It's possible to have a lot of evidence for a small effect. Also, a small coefficient doesn't even mean the interaction effect is small, since it is very sensitive to the units of the two variables.


## ISL Exercise 3.7.15 (20pts)

## Bonus question (20pts)

For multiple linear regression, show that $R^2$ is equal to the correlation between the response vector $\mathbf{y} = (y_1, \ldots, y_n)^T$ and the fitted values $\hat{\mathbf{y}} = (\hat y_1, \ldots, \hat y_n)^T$. That is
$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = [\operatorname{Cor}(\mathbf{y}, \hat{\mathbf{y}})]^2.
$$

